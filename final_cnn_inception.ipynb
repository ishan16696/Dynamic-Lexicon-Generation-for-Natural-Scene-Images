{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from pickle import dump, load\n",
    "from time import time\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96116736/96112376 [==============================] - 354s 4us/step\n"
     ]
    }
   ],
   "source": [
    "model = InceptionV3(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transform =  Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 299, 299, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_transform.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image_path):\n",
    "    # Convert all the images to size 299x299 as expected by the inception v3 model\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    # Convert PIL image to numpy array of 3-dimensions\n",
    "    x = image.img_to_array(img)\n",
    "    # Add one more dimension\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    # preprocess the images using preprocess_input() from inception module\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('test.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(image):\n",
    "    image = preprocess(image) # preprocess the image\n",
    "    fea_vec = model_transform.predict(image) # Get the encoding vector for the image\n",
    "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) # reshape from (1, 2048) to (2048, )\n",
    "    return fea_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=2048))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(10, input_dim=2048))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                65568     \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 65,898\n",
      "Trainable params: 65,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(key):\n",
    "    len1 = len(str(key))\n",
    "    str2 = \"0\"*(12-len1) + str(key)\n",
    "    return str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000000532481'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_path(532481)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(lda_result_file , directory_name ):\n",
    "    f = open(lda_result_file,\"rb\")\n",
    "    dict_lda = load(f)\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    #print(dict_lda)\n",
    "    for index,key in enumerate(dict_lda.keys()):\n",
    "        #print(\"key\",key,\"index\",index)\n",
    "        image_path = directory_name + \"/\" + get_path(key)+\".jpg\"\n",
    "        image_vector = encode(image_path)\n",
    "        train_x.append( list(image_vector))\n",
    "        train_y.append(list(dict_lda[key]))\n",
    "    train_x = np.array(train_x)\n",
    "    train_y = np.array(train_y)\n",
    "    \n",
    "    return train_x,train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x , train_y = prepare_training_data(\"ldaResult\",\"val2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YESSSSSSSSSSS\n"
     ]
    }
   ],
   "source": [
    "print(\"YESSSSSSSSSSS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"train_x\",\"wb\") as f:\n",
    "#     dump(train_x,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"train_y\",\"wb\") as f:\n",
    "#     dump(train_y,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_opt = optimizers.adam(lr=0.001)\n",
    "model.compile(optimizer=adam_opt , loss=\"categorical_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "5000/5000 [==============================] - 2s 371us/step - loss: 0.6910 - acc: 0.9040\n",
      "Epoch 2/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6920 - acc: 0.8970\n",
      "Epoch 3/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6909 - acc: 0.9000\n",
      "Epoch 4/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6812 - acc: 0.9096\n",
      "Epoch 5/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6849 - acc: 0.9022\n",
      "Epoch 6/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6839 - acc: 0.8984\n",
      "Epoch 7/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6806 - acc: 0.9070\n",
      "Epoch 8/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6907 - acc: 0.8928\n",
      "Epoch 9/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6835 - acc: 0.9014\n",
      "Epoch 10/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6790 - acc: 0.9010\n",
      "Epoch 11/1000\n",
      "5000/5000 [==============================] - 0s 81us/step - loss: 0.6808 - acc: 0.9004\n",
      "Epoch 12/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6730 - acc: 0.9112\n",
      "Epoch 13/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6691 - acc: 0.9054\n",
      "Epoch 14/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6671 - acc: 0.9056\n",
      "Epoch 15/1000\n",
      "5000/5000 [==============================] - 0s 80us/step - loss: 0.6755 - acc: 0.8982\n",
      "Epoch 16/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6709 - acc: 0.9030\n",
      "Epoch 17/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6699 - acc: 0.9044\n",
      "Epoch 18/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6794 - acc: 0.8846\n",
      "Epoch 19/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6724 - acc: 0.9024\n",
      "Epoch 20/1000\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 0.6661 - acc: 0.9028\n",
      "Epoch 21/1000\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 0.6644 - acc: 0.9078\n",
      "Epoch 22/1000\n",
      "5000/5000 [==============================] - 0s 80us/step - loss: 0.6617 - acc: 0.9072\n",
      "Epoch 23/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6677 - acc: 0.9018\n",
      "Epoch 24/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6590 - acc: 0.9126\n",
      "Epoch 25/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6537 - acc: 0.9104\n",
      "Epoch 26/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6562 - acc: 0.9050\n",
      "Epoch 27/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6643 - acc: 0.8988\n",
      "Epoch 28/1000\n",
      "5000/5000 [==============================] - 0s 81us/step - loss: 0.6622 - acc: 0.9024\n",
      "Epoch 29/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6601 - acc: 0.9040\n",
      "Epoch 30/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6609 - acc: 0.8968\n",
      "Epoch 31/1000\n",
      "5000/5000 [==============================] - 0s 82us/step - loss: 0.6582 - acc: 0.9014\n",
      "Epoch 32/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6589 - acc: 0.8974\n",
      "Epoch 33/1000\n",
      "5000/5000 [==============================] - 0s 82us/step - loss: 0.6530 - acc: 0.9060\n",
      "Epoch 34/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.6462 - acc: 0.9126\n",
      "Epoch 35/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6620 - acc: 0.8926\n",
      "Epoch 36/1000\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 0.6563 - acc: 0.9050\n",
      "Epoch 37/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6520 - acc: 0.9044\n",
      "Epoch 38/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6537 - acc: 0.9054\n",
      "Epoch 39/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6519 - acc: 0.9038\n",
      "Epoch 40/1000\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 0.6527 - acc: 0.9006\n",
      "Epoch 41/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.6538 - acc: 0.8978\n",
      "Epoch 42/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6526 - acc: 0.8974\n",
      "Epoch 43/1000\n",
      "5000/5000 [==============================] - 0s 81us/step - loss: 0.6497 - acc: 0.9038\n",
      "Epoch 44/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6495 - acc: 0.9022\n",
      "Epoch 45/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6561 - acc: 0.8968\n",
      "Epoch 46/1000\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 0.6474 - acc: 0.9062\n",
      "Epoch 47/1000\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 0.6457 - acc: 0.9076\n",
      "Epoch 48/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.6402 - acc: 0.9132\n",
      "Epoch 49/1000\n",
      "5000/5000 [==============================] - 0s 80us/step - loss: 0.6366 - acc: 0.9178\n",
      "Epoch 50/1000\n",
      "5000/5000 [==============================] - 0s 80us/step - loss: 0.6366 - acc: 0.9164\n",
      "Epoch 51/1000\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 0.6495 - acc: 0.8996\n",
      "Epoch 52/1000\n",
      "5000/5000 [==============================] - 0s 82us/step - loss: 0.6476 - acc: 0.9048\n",
      "Epoch 53/1000\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 0.6409 - acc: 0.9076\n",
      "Epoch 54/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.6401 - acc: 0.9096\n",
      "Epoch 55/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6584 - acc: 0.8832\n",
      "Epoch 56/1000\n",
      "5000/5000 [==============================] - 0s 82us/step - loss: 0.6546 - acc: 0.8918\n",
      "Epoch 57/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.6449 - acc: 0.9074\n",
      "Epoch 58/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.6401 - acc: 0.9054\n",
      "Epoch 59/1000\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 0.6377 - acc: 0.9092\n",
      "Epoch 60/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6406 - acc: 0.9112\n",
      "Epoch 61/1000\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 0.6389 - acc: 0.9110\n",
      "Epoch 62/1000\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 0.6348 - acc: 0.9164\n",
      "Epoch 63/1000\n",
      "5000/5000 [==============================] - 0s 80us/step - loss: 0.6363 - acc: 0.9106\n",
      "Epoch 64/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6336 - acc: 0.9144\n",
      "Epoch 65/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.6341 - acc: 0.9072\n",
      "Epoch 66/1000\n",
      "5000/5000 [==============================] - 0s 79us/step - loss: 0.6389 - acc: 0.9030\n",
      "Epoch 67/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6388 - acc: 0.9104\n",
      "Epoch 68/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.6384 - acc: 0.9044\n",
      "Epoch 69/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.6478 - acc: 0.8950\n",
      "Epoch 70/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.6357 - acc: 0.9060\n",
      "Epoch 71/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6315 - acc: 0.9162\n",
      "Epoch 72/1000\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 0.6440 - acc: 0.8976\n",
      "Epoch 73/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6331 - acc: 0.9128\n",
      "Epoch 74/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6352 - acc: 0.9120\n",
      "Epoch 75/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.6334 - acc: 0.9078\n",
      "Epoch 76/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6274 - acc: 0.9222\n",
      "Epoch 77/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6289 - acc: 0.9226\n",
      "Epoch 78/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.6307 - acc: 0.9130\n",
      "Epoch 79/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.6337 - acc: 0.9086\n",
      "Epoch 80/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.6425 - acc: 0.8948\n",
      "Epoch 81/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6383 - acc: 0.9042\n",
      "Epoch 82/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6363 - acc: 0.9000\n",
      "Epoch 83/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6335 - acc: 0.9002\n",
      "Epoch 84/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6309 - acc: 0.9096\n",
      "Epoch 85/1000\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 0.6411 - acc: 0.9036\n",
      "Epoch 86/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6273 - acc: 0.9192\n",
      "Epoch 87/1000\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 0.6298 - acc: 0.9158\n",
      "Epoch 88/1000\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 0.6285 - acc: 0.9110\n",
      "Epoch 89/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6262 - acc: 0.9122\n",
      "Epoch 90/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6308 - acc: 0.9112\n",
      "Epoch 91/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6278 - acc: 0.9142\n",
      "Epoch 92/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6346 - acc: 0.9064\n",
      "Epoch 93/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6257 - acc: 0.9166\n",
      "Epoch 94/1000\n",
      "5000/5000 [==============================] - 0s 80us/step - loss: 0.6286 - acc: 0.9064\n",
      "Epoch 95/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6283 - acc: 0.9138\n",
      "Epoch 96/1000\n",
      "5000/5000 [==============================] - 0s 81us/step - loss: 0.6257 - acc: 0.9120\n",
      "Epoch 97/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6259 - acc: 0.9190\n",
      "Epoch 98/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6318 - acc: 0.9046\n",
      "Epoch 99/1000\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 0.6321 - acc: 0.9044\n",
      "Epoch 100/1000\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 0.6255 - acc: 0.9126\n",
      "Epoch 101/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6234 - acc: 0.9134\n",
      "Epoch 102/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6232 - acc: 0.9202\n",
      "Epoch 103/1000\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 0.6252 - acc: 0.9134\n",
      "Epoch 104/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6272 - acc: 0.9130\n",
      "Epoch 105/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6283 - acc: 0.9076\n",
      "Epoch 106/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6256 - acc: 0.9150\n",
      "Epoch 107/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6214 - acc: 0.9158\n",
      "Epoch 108/1000\n",
      "5000/5000 [==============================] - 0s 85us/step - loss: 0.6216 - acc: 0.9164\n",
      "Epoch 109/1000\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 0.6221 - acc: 0.9130\n",
      "Epoch 110/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6203 - acc: 0.9224\n",
      "Epoch 111/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6254 - acc: 0.9084\n",
      "Epoch 112/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6273 - acc: 0.9094\n",
      "Epoch 113/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6238 - acc: 0.9096\n",
      "Epoch 114/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6246 - acc: 0.9112\n",
      "Epoch 115/1000\n",
      "5000/5000 [==============================] - 0s 81us/step - loss: 0.6254 - acc: 0.9106\n",
      "Epoch 116/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6208 - acc: 0.9134\n",
      "Epoch 117/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6197 - acc: 0.9156\n",
      "Epoch 118/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6196 - acc: 0.9200\n",
      "Epoch 119/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6224 - acc: 0.9140\n",
      "Epoch 120/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6266 - acc: 0.9054\n",
      "Epoch 121/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6248 - acc: 0.9062\n",
      "Epoch 122/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6191 - acc: 0.9152\n",
      "Epoch 123/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6180 - acc: 0.9196\n",
      "Epoch 124/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6180 - acc: 0.9168\n",
      "Epoch 125/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6195 - acc: 0.9146\n",
      "Epoch 126/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6167 - acc: 0.9282\n",
      "Epoch 127/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6192 - acc: 0.9126\n",
      "Epoch 128/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6191 - acc: 0.9184\n",
      "Epoch 129/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6253 - acc: 0.9062\n",
      "Epoch 130/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6220 - acc: 0.9122\n",
      "Epoch 131/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6188 - acc: 0.9138\n",
      "Epoch 132/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6233 - acc: 0.9068\n",
      "Epoch 133/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6187 - acc: 0.9204\n",
      "Epoch 134/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6195 - acc: 0.9134\n",
      "Epoch 135/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6163 - acc: 0.9192\n",
      "Epoch 136/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6156 - acc: 0.9216\n",
      "Epoch 137/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6158 - acc: 0.9172\n",
      "Epoch 138/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6195 - acc: 0.9144\n",
      "Epoch 139/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6195 - acc: 0.9142\n",
      "Epoch 140/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6272 - acc: 0.9056\n",
      "Epoch 141/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6245 - acc: 0.9104\n",
      "Epoch 142/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6237 - acc: 0.9022\n",
      "Epoch 143/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6183 - acc: 0.9182\n",
      "Epoch 144/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6138 - acc: 0.9260\n",
      "Epoch 145/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6112 - acc: 0.9278\n",
      "Epoch 146/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.6105 - acc: 0.9304\n",
      "Epoch 147/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6124 - acc: 0.9242\n",
      "Epoch 148/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6159 - acc: 0.9214\n",
      "Epoch 149/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6143 - acc: 0.9144\n",
      "Epoch 150/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6192 - acc: 0.9116\n",
      "Epoch 151/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6203 - acc: 0.9108\n",
      "Epoch 152/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6180 - acc: 0.9150\n",
      "Epoch 153/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6160 - acc: 0.9218\n",
      "Epoch 154/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6148 - acc: 0.9206\n",
      "Epoch 155/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6148 - acc: 0.9142\n",
      "Epoch 156/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6326 - acc: 0.8978\n",
      "Epoch 157/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6174 - acc: 0.9198\n",
      "Epoch 158/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.6090 - acc: 0.9306\n",
      "Epoch 159/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6088 - acc: 0.9280\n",
      "Epoch 160/1000\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 0.6086 - acc: 0.9358\n",
      "Epoch 161/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6082 - acc: 0.9274\n",
      "Epoch 162/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6116 - acc: 0.9256\n",
      "Epoch 163/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6146 - acc: 0.9164\n",
      "Epoch 164/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6245 - acc: 0.9018\n",
      "Epoch 165/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6203 - acc: 0.9116\n",
      "Epoch 166/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6208 - acc: 0.9050\n",
      "Epoch 167/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6131 - acc: 0.9154\n",
      "Epoch 168/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6097 - acc: 0.9288\n",
      "Epoch 169/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6102 - acc: 0.9276\n",
      "Epoch 170/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6110 - acc: 0.9272\n",
      "Epoch 171/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6126 - acc: 0.9200\n",
      "Epoch 172/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6135 - acc: 0.9204\n",
      "Epoch 173/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6218 - acc: 0.9076\n",
      "Epoch 174/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6175 - acc: 0.9122\n",
      "Epoch 175/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6172 - acc: 0.9142\n",
      "Epoch 176/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6150 - acc: 0.9180\n",
      "Epoch 177/1000\n",
      "5000/5000 [==============================] - 0s 62us/step - loss: 0.6118 - acc: 0.9180\n",
      "Epoch 178/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6118 - acc: 0.9240\n",
      "Epoch 179/1000\n",
      "5000/5000 [==============================] - 0s 62us/step - loss: 0.6106 - acc: 0.9238\n",
      "Epoch 180/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.6094 - acc: 0.9206\n",
      "Epoch 181/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6099 - acc: 0.9222\n",
      "Epoch 182/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6066 - acc: 0.9268\n",
      "Epoch 183/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6062 - acc: 0.9364\n",
      "Epoch 184/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6102 - acc: 0.9268\n",
      "Epoch 185/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6149 - acc: 0.9120\n",
      "Epoch 186/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6154 - acc: 0.9146\n",
      "Epoch 187/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6186 - acc: 0.9114\n",
      "Epoch 188/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6136 - acc: 0.9140\n",
      "Epoch 189/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6106 - acc: 0.9238\n",
      "Epoch 190/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6093 - acc: 0.9244\n",
      "Epoch 191/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6094 - acc: 0.9220\n",
      "Epoch 192/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6160 - acc: 0.9144\n",
      "Epoch 193/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6103 - acc: 0.9234\n",
      "Epoch 194/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6086 - acc: 0.9240\n",
      "Epoch 195/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6089 - acc: 0.9242\n",
      "Epoch 196/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6086 - acc: 0.9288\n",
      "Epoch 197/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6127 - acc: 0.9172\n",
      "Epoch 198/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6089 - acc: 0.9256\n",
      "Epoch 199/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6106 - acc: 0.9192\n",
      "Epoch 200/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6133 - acc: 0.9178\n",
      "Epoch 201/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6166 - acc: 0.9110\n",
      "Epoch 202/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6124 - acc: 0.9222\n",
      "Epoch 203/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6078 - acc: 0.9236\n",
      "Epoch 204/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6045 - acc: 0.9286\n",
      "Epoch 205/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6067 - acc: 0.9296\n",
      "Epoch 206/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6122 - acc: 0.9172\n",
      "Epoch 207/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6102 - acc: 0.9184\n",
      "Epoch 208/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6090 - acc: 0.9272\n",
      "Epoch 209/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6066 - acc: 0.9264\n",
      "Epoch 210/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6067 - acc: 0.9236\n",
      "Epoch 211/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6078 - acc: 0.9252\n",
      "Epoch 212/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6118 - acc: 0.9220\n",
      "Epoch 213/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6088 - acc: 0.9228\n",
      "Epoch 214/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6133 - acc: 0.9188\n",
      "Epoch 215/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6101 - acc: 0.9212\n",
      "Epoch 216/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6107 - acc: 0.9238\n",
      "Epoch 217/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6081 - acc: 0.9232\n",
      "Epoch 218/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6074 - acc: 0.9250\n",
      "Epoch 219/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6057 - acc: 0.9278\n",
      "Epoch 220/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6052 - acc: 0.9344\n",
      "Epoch 221/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6072 - acc: 0.9270\n",
      "Epoch 222/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6079 - acc: 0.9236\n",
      "Epoch 223/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6209 - acc: 0.9058\n",
      "Epoch 224/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6162 - acc: 0.9154\n",
      "Epoch 225/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6086 - acc: 0.9192\n",
      "Epoch 226/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6055 - acc: 0.9256\n",
      "Epoch 227/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6050 - acc: 0.9288\n",
      "Epoch 228/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6037 - acc: 0.9344\n",
      "Epoch 229/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6043 - acc: 0.9328\n",
      "Epoch 230/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6041 - acc: 0.9352\n",
      "Epoch 231/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6071 - acc: 0.9290\n",
      "Epoch 232/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6108 - acc: 0.9212\n",
      "Epoch 233/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6072 - acc: 0.9184\n",
      "Epoch 234/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6029 - acc: 0.9348\n",
      "Epoch 235/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6041 - acc: 0.9302\n",
      "Epoch 236/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6062 - acc: 0.9272\n",
      "Epoch 237/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6051 - acc: 0.9282\n",
      "Epoch 238/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6139 - acc: 0.9174\n",
      "Epoch 239/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6083 - acc: 0.9228\n",
      "Epoch 240/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6067 - acc: 0.9300\n",
      "Epoch 241/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6060 - acc: 0.9240\n",
      "Epoch 242/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6055 - acc: 0.9312\n",
      "Epoch 243/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6031 - acc: 0.9344\n",
      "Epoch 244/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6052 - acc: 0.9266\n",
      "Epoch 245/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6039 - acc: 0.9286\n",
      "Epoch 246/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6053 - acc: 0.9302\n",
      "Epoch 247/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6065 - acc: 0.9264\n",
      "Epoch 248/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6051 - acc: 0.9226\n",
      "Epoch 249/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6036 - acc: 0.9258\n",
      "Epoch 250/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6031 - acc: 0.9338\n",
      "Epoch 251/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6025 - acc: 0.9298\n",
      "Epoch 252/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6050 - acc: 0.9296\n",
      "Epoch 253/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6078 - acc: 0.9212\n",
      "Epoch 254/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6084 - acc: 0.9250\n",
      "Epoch 255/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6041 - acc: 0.9292\n",
      "Epoch 256/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6058 - acc: 0.9262\n",
      "Epoch 257/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6036 - acc: 0.9264\n",
      "Epoch 258/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6062 - acc: 0.9258\n",
      "Epoch 259/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6049 - acc: 0.9276\n",
      "Epoch 260/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6055 - acc: 0.9194\n",
      "Epoch 261/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6101 - acc: 0.9198\n",
      "Epoch 262/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6105 - acc: 0.9200\n",
      "Epoch 263/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6046 - acc: 0.9252\n",
      "Epoch 264/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6065 - acc: 0.9262\n",
      "Epoch 265/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6016 - acc: 0.9320\n",
      "Epoch 266/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5998 - acc: 0.9332\n",
      "Epoch 267/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6082 - acc: 0.9230\n",
      "Epoch 268/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6029 - acc: 0.9292\n",
      "Epoch 269/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6024 - acc: 0.9296\n",
      "Epoch 270/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6017 - acc: 0.9338\n",
      "Epoch 271/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6061 - acc: 0.9254\n",
      "Epoch 272/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6066 - acc: 0.9234\n",
      "Epoch 273/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6175 - acc: 0.9058\n",
      "Epoch 274/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6036 - acc: 0.9294\n",
      "Epoch 275/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6036 - acc: 0.9276\n",
      "Epoch 276/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6006 - acc: 0.9314\n",
      "Epoch 277/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5995 - acc: 0.9354\n",
      "Epoch 278/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5989 - acc: 0.9424\n",
      "Epoch 279/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6012 - acc: 0.9332\n",
      "Epoch 280/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6032 - acc: 0.9286\n",
      "Epoch 281/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6032 - acc: 0.9280\n",
      "Epoch 282/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6017 - acc: 0.9324\n",
      "Epoch 283/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6085 - acc: 0.9252\n",
      "Epoch 284/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6065 - acc: 0.9230\n",
      "Epoch 285/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6053 - acc: 0.9192\n",
      "Epoch 286/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6031 - acc: 0.9284\n",
      "Epoch 287/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5998 - acc: 0.9352\n",
      "Epoch 288/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6025 - acc: 0.9334\n",
      "Epoch 289/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6017 - acc: 0.9300\n",
      "Epoch 290/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6057 - acc: 0.9268\n",
      "Epoch 291/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6102 - acc: 0.9094\n",
      "Epoch 292/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6068 - acc: 0.9252\n",
      "Epoch 293/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6042 - acc: 0.9248\n",
      "Epoch 294/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6011 - acc: 0.9298\n",
      "Epoch 295/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5984 - acc: 0.9432\n",
      "Epoch 296/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6014 - acc: 0.9366\n",
      "Epoch 297/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6024 - acc: 0.9280\n",
      "Epoch 298/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6035 - acc: 0.9312\n",
      "Epoch 299/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6022 - acc: 0.9318\n",
      "Epoch 300/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6036 - acc: 0.9222\n",
      "Epoch 301/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6102 - acc: 0.9194\n",
      "Epoch 302/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6054 - acc: 0.9252\n",
      "Epoch 303/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6020 - acc: 0.9310\n",
      "Epoch 304/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6010 - acc: 0.9348\n",
      "Epoch 305/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.6002 - acc: 0.9348\n",
      "Epoch 306/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5996 - acc: 0.9368\n",
      "Epoch 307/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6002 - acc: 0.9334\n",
      "Epoch 308/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6002 - acc: 0.9378\n",
      "Epoch 309/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6025 - acc: 0.9270\n",
      "Epoch 310/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6058 - acc: 0.9196\n",
      "Epoch 311/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6037 - acc: 0.9246\n",
      "Epoch 312/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6019 - acc: 0.9266\n",
      "Epoch 313/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6020 - acc: 0.9328\n",
      "Epoch 314/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6051 - acc: 0.9198\n",
      "Epoch 315/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6015 - acc: 0.9318\n",
      "Epoch 316/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5982 - acc: 0.9386\n",
      "Epoch 317/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5998 - acc: 0.9350\n",
      "Epoch 318/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6010 - acc: 0.9324\n",
      "Epoch 319/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6019 - acc: 0.9292\n",
      "Epoch 320/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6024 - acc: 0.9264\n",
      "Epoch 321/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5974 - acc: 0.9352\n",
      "Epoch 322/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5990 - acc: 0.9354\n",
      "Epoch 323/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6059 - acc: 0.9258\n",
      "Epoch 324/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6054 - acc: 0.9176\n",
      "Epoch 325/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6044 - acc: 0.9256\n",
      "Epoch 326/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6052 - acc: 0.9266\n",
      "Epoch 327/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6009 - acc: 0.9326\n",
      "Epoch 328/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5998 - acc: 0.9346\n",
      "Epoch 329/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5981 - acc: 0.9310\n",
      "Epoch 330/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5973 - acc: 0.9406\n",
      "Epoch 331/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6003 - acc: 0.9284\n",
      "Epoch 332/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6007 - acc: 0.9322\n",
      "Epoch 333/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6011 - acc: 0.9332\n",
      "Epoch 334/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6040 - acc: 0.9278\n",
      "Epoch 335/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6096 - acc: 0.9150\n",
      "Epoch 336/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6032 - acc: 0.9258\n",
      "Epoch 337/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5996 - acc: 0.9372\n",
      "Epoch 338/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6009 - acc: 0.9324\n",
      "Epoch 339/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6028 - acc: 0.9262\n",
      "Epoch 340/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6008 - acc: 0.9334\n",
      "Epoch 341/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6002 - acc: 0.9366\n",
      "Epoch 342/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5984 - acc: 0.9330\n",
      "Epoch 343/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6008 - acc: 0.9328\n",
      "Epoch 344/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6018 - acc: 0.9290\n",
      "Epoch 345/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5996 - acc: 0.9332\n",
      "Epoch 346/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5996 - acc: 0.9336\n",
      "Epoch 347/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5981 - acc: 0.9350\n",
      "Epoch 348/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6045 - acc: 0.9222\n",
      "Epoch 349/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.6060 - acc: 0.9234\n",
      "Epoch 350/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6034 - acc: 0.9238\n",
      "Epoch 351/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6062 - acc: 0.9232\n",
      "Epoch 352/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6021 - acc: 0.9270\n",
      "Epoch 353/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5978 - acc: 0.9376\n",
      "Epoch 354/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5981 - acc: 0.9382\n",
      "Epoch 355/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5973 - acc: 0.9420\n",
      "Epoch 356/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6019 - acc: 0.9220\n",
      "Epoch 357/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6015 - acc: 0.9254\n",
      "Epoch 358/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6014 - acc: 0.9284\n",
      "Epoch 359/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5986 - acc: 0.9390\n",
      "Epoch 360/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5972 - acc: 0.9356\n",
      "Epoch 361/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5961 - acc: 0.9436\n",
      "Epoch 362/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5995 - acc: 0.9266\n",
      "Epoch 363/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6019 - acc: 0.9280\n",
      "Epoch 364/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6012 - acc: 0.9364\n",
      "Epoch 365/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6016 - acc: 0.9290\n",
      "Epoch 366/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5980 - acc: 0.9372\n",
      "Epoch 367/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5983 - acc: 0.9412\n",
      "Epoch 368/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6012 - acc: 0.9330\n",
      "Epoch 369/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6026 - acc: 0.9268\n",
      "Epoch 370/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6038 - acc: 0.9292\n",
      "Epoch 371/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6023 - acc: 0.9292\n",
      "Epoch 372/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6009 - acc: 0.9340\n",
      "Epoch 373/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6007 - acc: 0.9312\n",
      "Epoch 374/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6037 - acc: 0.9306\n",
      "Epoch 375/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6016 - acc: 0.9282\n",
      "Epoch 376/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6005 - acc: 0.9312\n",
      "Epoch 377/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5986 - acc: 0.9374\n",
      "Epoch 378/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5963 - acc: 0.9430\n",
      "Epoch 379/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5962 - acc: 0.9432\n",
      "Epoch 380/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5994 - acc: 0.9322\n",
      "Epoch 381/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6065 - acc: 0.9184\n",
      "Epoch 382/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6058 - acc: 0.9224\n",
      "Epoch 383/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6003 - acc: 0.9292\n",
      "Epoch 384/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5981 - acc: 0.9384\n",
      "Epoch 385/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5972 - acc: 0.9412\n",
      "Epoch 386/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5966 - acc: 0.9384\n",
      "Epoch 387/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5981 - acc: 0.9364\n",
      "Epoch 388/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6000 - acc: 0.9336\n",
      "Epoch 389/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5994 - acc: 0.9346\n",
      "Epoch 390/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5989 - acc: 0.9330\n",
      "Epoch 391/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6010 - acc: 0.9328\n",
      "Epoch 392/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5994 - acc: 0.9314\n",
      "Epoch 393/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5981 - acc: 0.9348\n",
      "Epoch 394/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5965 - acc: 0.9404\n",
      "Epoch 395/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5961 - acc: 0.9408\n",
      "Epoch 396/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6017 - acc: 0.9306\n",
      "Epoch 397/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6015 - acc: 0.9280\n",
      "Epoch 398/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6101 - acc: 0.9130\n",
      "Epoch 399/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6081 - acc: 0.9212\n",
      "Epoch 400/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6005 - acc: 0.9220\n",
      "Epoch 401/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5971 - acc: 0.9412\n",
      "Epoch 402/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5987 - acc: 0.9298\n",
      "Epoch 403/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5991 - acc: 0.9324\n",
      "Epoch 404/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5964 - acc: 0.9408\n",
      "Epoch 405/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5987 - acc: 0.9366\n",
      "Epoch 406/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5974 - acc: 0.9350\n",
      "Epoch 407/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5982 - acc: 0.9342\n",
      "Epoch 408/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5970 - acc: 0.9396\n",
      "Epoch 409/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5971 - acc: 0.9350\n",
      "Epoch 410/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5968 - acc: 0.9418\n",
      "Epoch 411/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5988 - acc: 0.9356\n",
      "Epoch 412/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6039 - acc: 0.9258\n",
      "Epoch 413/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6136 - acc: 0.9088\n",
      "Epoch 414/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6038 - acc: 0.9254\n",
      "Epoch 415/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6008 - acc: 0.9310\n",
      "Epoch 416/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5966 - acc: 0.9384\n",
      "Epoch 417/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5948 - acc: 0.9422\n",
      "Epoch 418/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5984 - acc: 0.9368\n",
      "Epoch 419/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5964 - acc: 0.9394\n",
      "Epoch 420/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5958 - acc: 0.9370\n",
      "Epoch 421/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5994 - acc: 0.9356\n",
      "Epoch 422/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5982 - acc: 0.9354\n",
      "Epoch 423/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6016 - acc: 0.9306\n",
      "Epoch 424/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6013 - acc: 0.9282\n",
      "Epoch 425/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5989 - acc: 0.9336\n",
      "Epoch 426/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5968 - acc: 0.9412\n",
      "Epoch 427/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5992 - acc: 0.9258\n",
      "Epoch 428/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5974 - acc: 0.9376\n",
      "Epoch 429/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5971 - acc: 0.9332\n",
      "Epoch 430/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5998 - acc: 0.9330\n",
      "Epoch 431/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5991 - acc: 0.9346\n",
      "Epoch 432/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5977 - acc: 0.9374\n",
      "Epoch 433/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5955 - acc: 0.9386\n",
      "Epoch 434/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5973 - acc: 0.9346\n",
      "Epoch 435/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5990 - acc: 0.9324\n",
      "Epoch 436/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6071 - acc: 0.9180\n",
      "Epoch 437/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6101 - acc: 0.9160\n",
      "Epoch 438/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6005 - acc: 0.9294\n",
      "Epoch 439/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5966 - acc: 0.9386\n",
      "Epoch 440/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5936 - acc: 0.9458\n",
      "Epoch 441/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5935 - acc: 0.9446\n",
      "Epoch 442/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5963 - acc: 0.9412\n",
      "Epoch 443/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5945 - acc: 0.9442\n",
      "Epoch 444/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5979 - acc: 0.9342\n",
      "Epoch 445/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5991 - acc: 0.9392\n",
      "Epoch 446/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6014 - acc: 0.9234\n",
      "Epoch 447/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6015 - acc: 0.9216\n",
      "Epoch 448/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6021 - acc: 0.9284\n",
      "Epoch 449/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6034 - acc: 0.9184\n",
      "Epoch 450/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5997 - acc: 0.9310\n",
      "Epoch 451/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5978 - acc: 0.9338\n",
      "Epoch 452/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5980 - acc: 0.9356\n",
      "Epoch 453/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5955 - acc: 0.9354\n",
      "Epoch 454/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5959 - acc: 0.9388\n",
      "Epoch 455/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5985 - acc: 0.9302\n",
      "Epoch 456/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6032 - acc: 0.9220\n",
      "Epoch 457/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5978 - acc: 0.9400\n",
      "Epoch 458/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5977 - acc: 0.9340\n",
      "Epoch 459/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5953 - acc: 0.9406\n",
      "Epoch 460/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5958 - acc: 0.9394\n",
      "Epoch 461/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5984 - acc: 0.9292\n",
      "Epoch 462/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5974 - acc: 0.9366\n",
      "Epoch 463/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5964 - acc: 0.9378\n",
      "Epoch 464/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5987 - acc: 0.9366\n",
      "Epoch 465/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5994 - acc: 0.9316\n",
      "Epoch 466/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5993 - acc: 0.9338\n",
      "Epoch 467/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6027 - acc: 0.9264\n",
      "Epoch 468/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5983 - acc: 0.9336\n",
      "Epoch 469/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5983 - acc: 0.9290\n",
      "Epoch 470/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5955 - acc: 0.9380\n",
      "Epoch 471/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5968 - acc: 0.9356\n",
      "Epoch 472/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5951 - acc: 0.9402\n",
      "Epoch 473/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6028 - acc: 0.9244\n",
      "Epoch 474/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6042 - acc: 0.9164\n",
      "Epoch 475/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5985 - acc: 0.9346\n",
      "Epoch 476/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5948 - acc: 0.9412\n",
      "Epoch 477/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5978 - acc: 0.9350\n",
      "Epoch 478/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5985 - acc: 0.9302\n",
      "Epoch 479/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5958 - acc: 0.9376\n",
      "Epoch 480/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5952 - acc: 0.9384\n",
      "Epoch 481/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5966 - acc: 0.9368\n",
      "Epoch 482/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5971 - acc: 0.9348\n",
      "Epoch 483/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6019 - acc: 0.9238\n",
      "Epoch 484/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6007 - acc: 0.9296\n",
      "Epoch 485/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5991 - acc: 0.9348\n",
      "Epoch 486/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6005 - acc: 0.9270\n",
      "Epoch 487/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5975 - acc: 0.9366\n",
      "Epoch 488/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5958 - acc: 0.9376\n",
      "Epoch 489/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5941 - acc: 0.9412\n",
      "Epoch 490/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5965 - acc: 0.9346\n",
      "Epoch 491/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6008 - acc: 0.9272\n",
      "Epoch 492/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5983 - acc: 0.9340\n",
      "Epoch 493/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5979 - acc: 0.9334\n",
      "Epoch 494/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6034 - acc: 0.9254\n",
      "Epoch 495/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5983 - acc: 0.9370\n",
      "Epoch 496/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5960 - acc: 0.9368\n",
      "Epoch 497/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5993 - acc: 0.9328\n",
      "Epoch 498/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5975 - acc: 0.9348\n",
      "Epoch 499/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5978 - acc: 0.9404\n",
      "Epoch 500/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5965 - acc: 0.9384\n",
      "Epoch 501/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5974 - acc: 0.9322\n",
      "Epoch 502/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5957 - acc: 0.9392\n",
      "Epoch 503/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5952 - acc: 0.9400\n",
      "Epoch 504/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5968 - acc: 0.9342\n",
      "Epoch 505/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5973 - acc: 0.9326\n",
      "Epoch 506/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6017 - acc: 0.9298\n",
      "Epoch 507/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6022 - acc: 0.9280\n",
      "Epoch 508/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5986 - acc: 0.9358\n",
      "Epoch 509/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5948 - acc: 0.9422\n",
      "Epoch 510/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5961 - acc: 0.9384\n",
      "Epoch 511/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5950 - acc: 0.9410\n",
      "Epoch 512/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5974 - acc: 0.9358\n",
      "Epoch 513/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5996 - acc: 0.9330\n",
      "Epoch 514/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5972 - acc: 0.9324\n",
      "Epoch 515/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5969 - acc: 0.9318\n",
      "Epoch 516/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5957 - acc: 0.9420\n",
      "Epoch 517/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6007 - acc: 0.9290\n",
      "Epoch 518/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6002 - acc: 0.9278\n",
      "Epoch 519/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5980 - acc: 0.9308\n",
      "Epoch 520/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5968 - acc: 0.9366\n",
      "Epoch 521/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5958 - acc: 0.9370\n",
      "Epoch 522/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5999 - acc: 0.9338\n",
      "Epoch 523/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5954 - acc: 0.9338\n",
      "Epoch 524/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5967 - acc: 0.9348\n",
      "Epoch 525/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6029 - acc: 0.9220\n",
      "Epoch 526/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5992 - acc: 0.9316\n",
      "Epoch 527/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5953 - acc: 0.9382\n",
      "Epoch 528/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5955 - acc: 0.9394\n",
      "Epoch 529/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5953 - acc: 0.9368\n",
      "Epoch 530/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5963 - acc: 0.9382\n",
      "Epoch 531/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6038 - acc: 0.9250\n",
      "Epoch 532/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5986 - acc: 0.9316\n",
      "Epoch 533/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5995 - acc: 0.9316\n",
      "Epoch 534/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5983 - acc: 0.9310\n",
      "Epoch 535/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5947 - acc: 0.9422\n",
      "Epoch 536/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5951 - acc: 0.9388\n",
      "Epoch 537/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5952 - acc: 0.9406\n",
      "Epoch 538/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5978 - acc: 0.9352\n",
      "Epoch 539/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5952 - acc: 0.9382\n",
      "Epoch 540/1000\n",
      "5000/5000 [==============================] - 0s 58us/step - loss: 0.5965 - acc: 0.9368\n",
      "Epoch 541/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5987 - acc: 0.9282\n",
      "Epoch 542/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.6001 - acc: 0.9266\n",
      "Epoch 543/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5980 - acc: 0.9342\n",
      "Epoch 544/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5954 - acc: 0.9356\n",
      "Epoch 545/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5948 - acc: 0.9392\n",
      "Epoch 546/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5959 - acc: 0.9392\n",
      "Epoch 547/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5953 - acc: 0.9400\n",
      "Epoch 548/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5955 - acc: 0.9360\n",
      "Epoch 549/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5948 - acc: 0.9394\n",
      "Epoch 550/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5962 - acc: 0.9394\n",
      "Epoch 551/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5975 - acc: 0.9358\n",
      "Epoch 552/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5966 - acc: 0.9310\n",
      "Epoch 553/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5979 - acc: 0.9352\n",
      "Epoch 554/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5984 - acc: 0.9354\n",
      "Epoch 555/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5985 - acc: 0.9284\n",
      "Epoch 556/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5996 - acc: 0.9300\n",
      "Epoch 557/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.6014 - acc: 0.9230\n",
      "Epoch 558/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5954 - acc: 0.9376\n",
      "Epoch 559/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5948 - acc: 0.9450\n",
      "Epoch 560/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5935 - acc: 0.9404\n",
      "Epoch 561/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5962 - acc: 0.9382\n",
      "Epoch 562/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5951 - acc: 0.9430\n",
      "Epoch 563/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5961 - acc: 0.9378\n",
      "Epoch 564/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6037 - acc: 0.9216\n",
      "Epoch 565/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5990 - acc: 0.9312\n",
      "Epoch 566/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5955 - acc: 0.9370\n",
      "Epoch 567/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5983 - acc: 0.9340\n",
      "Epoch 568/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5998 - acc: 0.9288\n",
      "Epoch 569/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5968 - acc: 0.9356\n",
      "Epoch 570/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5957 - acc: 0.9422\n",
      "Epoch 571/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5949 - acc: 0.9350\n",
      "Epoch 572/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5970 - acc: 0.9362\n",
      "Epoch 573/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5980 - acc: 0.9328\n",
      "Epoch 574/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5953 - acc: 0.9406\n",
      "Epoch 575/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5946 - acc: 0.9426\n",
      "Epoch 576/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5955 - acc: 0.9398\n",
      "Epoch 577/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5996 - acc: 0.9332\n",
      "Epoch 578/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5973 - acc: 0.9354\n",
      "Epoch 579/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6001 - acc: 0.9308\n",
      "Epoch 580/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5979 - acc: 0.9310\n",
      "Epoch 581/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5950 - acc: 0.9368\n",
      "Epoch 582/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5960 - acc: 0.9396\n",
      "Epoch 583/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5973 - acc: 0.9346\n",
      "Epoch 584/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5993 - acc: 0.9276\n",
      "Epoch 585/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6000 - acc: 0.9292\n",
      "Epoch 586/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5990 - acc: 0.9300\n",
      "Epoch 587/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5953 - acc: 0.9406\n",
      "Epoch 588/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5949 - acc: 0.9412\n",
      "Epoch 589/1000\n",
      "5000/5000 [==============================] - 1s 108us/step - loss: 0.5939 - acc: 0.9414\n",
      "Epoch 590/1000\n",
      "5000/5000 [==============================] - 1s 125us/step - loss: 0.5947 - acc: 0.9396\n",
      "Epoch 591/1000\n",
      "5000/5000 [==============================] - 1s 119us/step - loss: 0.5956 - acc: 0.9396\n",
      "Epoch 592/1000\n",
      "5000/5000 [==============================] - 1s 117us/step - loss: 0.5946 - acc: 0.9424\n",
      "Epoch 593/1000\n",
      "5000/5000 [==============================] - 1s 121us/step - loss: 0.5953 - acc: 0.9364\n",
      "Epoch 594/1000\n",
      "5000/5000 [==============================] - 1s 118us/step - loss: 0.5962 - acc: 0.9378\n",
      "Epoch 595/1000\n",
      "5000/5000 [==============================] - 1s 109us/step - loss: 0.5974 - acc: 0.9388\n",
      "Epoch 596/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5968 - acc: 0.9318\n",
      "Epoch 597/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5977 - acc: 0.9312\n",
      "Epoch 598/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.6011 - acc: 0.9288\n",
      "Epoch 599/1000\n",
      "5000/5000 [==============================] - 0s 94us/step - loss: 0.5974 - acc: 0.9328\n",
      "Epoch 600/1000\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 0.5956 - acc: 0.9370\n",
      "Epoch 601/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5960 - acc: 0.9370\n",
      "Epoch 602/1000\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 0.5945 - acc: 0.9382\n",
      "Epoch 603/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.5959 - acc: 0.9382\n",
      "Epoch 604/1000\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 0.5943 - acc: 0.9350\n",
      "Epoch 605/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5968 - acc: 0.9354\n",
      "Epoch 606/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5966 - acc: 0.9412\n",
      "Epoch 607/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6003 - acc: 0.9250\n",
      "Epoch 608/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5971 - acc: 0.9354\n",
      "Epoch 609/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5990 - acc: 0.9316\n",
      "Epoch 610/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5969 - acc: 0.9356\n",
      "Epoch 611/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5962 - acc: 0.9344\n",
      "Epoch 612/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5950 - acc: 0.9406\n",
      "Epoch 613/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5943 - acc: 0.9376\n",
      "Epoch 614/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5988 - acc: 0.9336\n",
      "Epoch 615/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5965 - acc: 0.9386\n",
      "Epoch 616/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5965 - acc: 0.9420\n",
      "Epoch 617/1000\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 0.5953 - acc: 0.9404\n",
      "Epoch 618/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5956 - acc: 0.9396\n",
      "Epoch 619/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5942 - acc: 0.9396\n",
      "Epoch 620/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5976 - acc: 0.9342\n",
      "Epoch 621/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.6056 - acc: 0.9216\n",
      "Epoch 622/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5979 - acc: 0.9352\n",
      "Epoch 623/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5996 - acc: 0.9318\n",
      "Epoch 624/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5971 - acc: 0.9294\n",
      "Epoch 625/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5943 - acc: 0.9434\n",
      "Epoch 626/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5948 - acc: 0.9358\n",
      "Epoch 627/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5936 - acc: 0.9384\n",
      "Epoch 628/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5944 - acc: 0.9432\n",
      "Epoch 629/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5956 - acc: 0.9382\n",
      "Epoch 630/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5944 - acc: 0.9390\n",
      "Epoch 631/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5963 - acc: 0.9346\n",
      "Epoch 632/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5947 - acc: 0.9416\n",
      "Epoch 633/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5944 - acc: 0.9366\n",
      "Epoch 634/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5968 - acc: 0.9338\n",
      "Epoch 635/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5959 - acc: 0.9392\n",
      "Epoch 636/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5972 - acc: 0.9310\n",
      "Epoch 637/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5981 - acc: 0.9306\n",
      "Epoch 638/1000\n",
      "5000/5000 [==============================] - 0s 92us/step - loss: 0.5967 - acc: 0.9386\n",
      "Epoch 639/1000\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 0.5953 - acc: 0.9350\n",
      "Epoch 640/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5977 - acc: 0.9366\n",
      "Epoch 641/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.5969 - acc: 0.9344\n",
      "Epoch 642/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5942 - acc: 0.9408\n",
      "Epoch 643/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5958 - acc: 0.9352\n",
      "Epoch 644/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5950 - acc: 0.9440\n",
      "Epoch 645/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5971 - acc: 0.9374\n",
      "Epoch 646/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5992 - acc: 0.9336\n",
      "Epoch 647/1000\n",
      "5000/5000 [==============================] - 0s 79us/step - loss: 0.5973 - acc: 0.9346\n",
      "Epoch 648/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5958 - acc: 0.9332\n",
      "Epoch 649/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5962 - acc: 0.9394\n",
      "Epoch 650/1000\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 0.5959 - acc: 0.9410\n",
      "Epoch 651/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5962 - acc: 0.9378\n",
      "Epoch 652/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5945 - acc: 0.9412\n",
      "Epoch 653/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5935 - acc: 0.9400\n",
      "Epoch 654/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5953 - acc: 0.9362\n",
      "Epoch 655/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5958 - acc: 0.9366\n",
      "Epoch 656/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6000 - acc: 0.9294\n",
      "Epoch 657/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.6003 - acc: 0.9212\n",
      "Epoch 658/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5976 - acc: 0.9356\n",
      "Epoch 659/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5960 - acc: 0.9374\n",
      "Epoch 660/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5953 - acc: 0.9376\n",
      "Epoch 661/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5945 - acc: 0.9434\n",
      "Epoch 662/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5940 - acc: 0.9448\n",
      "Epoch 663/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5985 - acc: 0.9322\n",
      "Epoch 664/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5967 - acc: 0.9360\n",
      "Epoch 665/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5953 - acc: 0.9404\n",
      "Epoch 666/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5936 - acc: 0.9406\n",
      "Epoch 667/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5929 - acc: 0.9428\n",
      "Epoch 668/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5961 - acc: 0.9384\n",
      "Epoch 669/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5959 - acc: 0.9350\n",
      "Epoch 670/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5945 - acc: 0.9398\n",
      "Epoch 671/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5966 - acc: 0.9368\n",
      "Epoch 672/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5967 - acc: 0.9342\n",
      "Epoch 673/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5970 - acc: 0.9388\n",
      "Epoch 674/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5940 - acc: 0.9406\n",
      "Epoch 675/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5943 - acc: 0.9410\n",
      "Epoch 676/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5959 - acc: 0.9364\n",
      "Epoch 677/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.6041 - acc: 0.9214\n",
      "Epoch 678/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5976 - acc: 0.9306\n",
      "Epoch 679/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5987 - acc: 0.9326\n",
      "Epoch 680/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5944 - acc: 0.9406\n",
      "Epoch 681/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5951 - acc: 0.9400\n",
      "Epoch 682/1000\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 0.5950 - acc: 0.9380\n",
      "Epoch 683/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5954 - acc: 0.9396\n",
      "Epoch 684/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5968 - acc: 0.9334\n",
      "Epoch 685/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5947 - acc: 0.9424\n",
      "Epoch 686/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5978 - acc: 0.9352\n",
      "Epoch 687/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5972 - acc: 0.9278\n",
      "Epoch 688/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5947 - acc: 0.9428\n",
      "Epoch 689/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5941 - acc: 0.9360\n",
      "Epoch 690/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5931 - acc: 0.9438\n",
      "Epoch 691/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5943 - acc: 0.9450\n",
      "Epoch 692/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5998 - acc: 0.9318\n",
      "Epoch 693/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5963 - acc: 0.9390\n",
      "Epoch 694/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5980 - acc: 0.9376\n",
      "Epoch 695/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5955 - acc: 0.9382\n",
      "Epoch 696/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5941 - acc: 0.9414\n",
      "Epoch 697/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5948 - acc: 0.9386\n",
      "Epoch 698/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5947 - acc: 0.9384\n",
      "Epoch 699/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5949 - acc: 0.9388\n",
      "Epoch 700/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5980 - acc: 0.9314\n",
      "Epoch 701/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5971 - acc: 0.9382\n",
      "Epoch 702/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5983 - acc: 0.9286\n",
      "Epoch 703/1000\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 0.5943 - acc: 0.9404\n",
      "Epoch 704/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5943 - acc: 0.9436\n",
      "Epoch 705/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6080 - acc: 0.9256\n",
      "Epoch 706/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5955 - acc: 0.9422\n",
      "Epoch 707/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5958 - acc: 0.9346\n",
      "Epoch 708/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5942 - acc: 0.9390\n",
      "Epoch 709/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5941 - acc: 0.9456\n",
      "Epoch 710/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5938 - acc: 0.9440\n",
      "Epoch 711/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5955 - acc: 0.9438\n",
      "Epoch 712/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5973 - acc: 0.9320\n",
      "Epoch 713/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5967 - acc: 0.9416\n",
      "Epoch 714/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5974 - acc: 0.9354\n",
      "Epoch 715/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5965 - acc: 0.9390\n",
      "Epoch 716/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5987 - acc: 0.9310\n",
      "Epoch 717/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5965 - acc: 0.9386\n",
      "Epoch 718/1000\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 0.5942 - acc: 0.9388\n",
      "Epoch 719/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5947 - acc: 0.9436\n",
      "Epoch 720/1000\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 0.5950 - acc: 0.9406\n",
      "Epoch 721/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5941 - acc: 0.9398\n",
      "Epoch 722/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5949 - acc: 0.9398\n",
      "Epoch 723/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5942 - acc: 0.9440\n",
      "Epoch 724/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5946 - acc: 0.9370\n",
      "Epoch 725/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5945 - acc: 0.9392\n",
      "Epoch 726/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5960 - acc: 0.9370\n",
      "Epoch 727/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5948 - acc: 0.9374\n",
      "Epoch 728/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5988 - acc: 0.9274\n",
      "Epoch 729/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5976 - acc: 0.9390\n",
      "Epoch 730/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5978 - acc: 0.9358\n",
      "Epoch 731/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5948 - acc: 0.9412\n",
      "Epoch 732/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5936 - acc: 0.9440\n",
      "Epoch 733/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5955 - acc: 0.9382\n",
      "Epoch 734/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5945 - acc: 0.9394\n",
      "Epoch 735/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5946 - acc: 0.9394\n",
      "Epoch 736/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5984 - acc: 0.9348\n",
      "Epoch 737/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5950 - acc: 0.9398\n",
      "Epoch 738/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5957 - acc: 0.9400\n",
      "Epoch 739/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5968 - acc: 0.9320\n",
      "Epoch 740/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5992 - acc: 0.9276\n",
      "Epoch 741/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5956 - acc: 0.9400\n",
      "Epoch 742/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5952 - acc: 0.9382\n",
      "Epoch 743/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5944 - acc: 0.9394\n",
      "Epoch 744/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5943 - acc: 0.9396\n",
      "Epoch 745/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5961 - acc: 0.9348\n",
      "Epoch 746/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5971 - acc: 0.9318\n",
      "Epoch 747/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5970 - acc: 0.9352\n",
      "Epoch 748/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5946 - acc: 0.9418\n",
      "Epoch 749/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5939 - acc: 0.9368\n",
      "Epoch 750/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5956 - acc: 0.9358\n",
      "Epoch 751/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5975 - acc: 0.9362\n",
      "Epoch 752/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5951 - acc: 0.9410\n",
      "Epoch 753/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5990 - acc: 0.9358\n",
      "Epoch 754/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5964 - acc: 0.9338\n",
      "Epoch 755/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5935 - acc: 0.9384\n",
      "Epoch 756/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5938 - acc: 0.9426\n",
      "Epoch 757/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5947 - acc: 0.9396\n",
      "Epoch 758/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5948 - acc: 0.9338\n",
      "Epoch 759/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5947 - acc: 0.9382\n",
      "Epoch 760/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5947 - acc: 0.9408\n",
      "Epoch 761/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5964 - acc: 0.9320\n",
      "Epoch 762/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5963 - acc: 0.9354\n",
      "Epoch 763/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5980 - acc: 0.9306\n",
      "Epoch 764/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5957 - acc: 0.9390\n",
      "Epoch 765/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5971 - acc: 0.9312\n",
      "Epoch 766/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5946 - acc: 0.9366\n",
      "Epoch 767/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5944 - acc: 0.9382\n",
      "Epoch 768/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5956 - acc: 0.9398\n",
      "Epoch 769/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5963 - acc: 0.9390\n",
      "Epoch 770/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5939 - acc: 0.9398\n",
      "Epoch 771/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5951 - acc: 0.9388\n",
      "Epoch 772/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5963 - acc: 0.9398\n",
      "Epoch 773/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5944 - acc: 0.9358\n",
      "Epoch 774/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5945 - acc: 0.9384\n",
      "Epoch 775/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5971 - acc: 0.9350\n",
      "Epoch 776/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6022 - acc: 0.9272\n",
      "Epoch 777/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5959 - acc: 0.9394\n",
      "Epoch 778/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5960 - acc: 0.9370\n",
      "Epoch 779/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5944 - acc: 0.9378\n",
      "Epoch 780/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5957 - acc: 0.9354\n",
      "Epoch 781/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5983 - acc: 0.9324\n",
      "Epoch 782/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5945 - acc: 0.9364\n",
      "Epoch 783/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5934 - acc: 0.9432\n",
      "Epoch 784/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5950 - acc: 0.9428\n",
      "Epoch 785/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5935 - acc: 0.9392\n",
      "Epoch 786/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5948 - acc: 0.9368\n",
      "Epoch 787/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5950 - acc: 0.9376\n",
      "Epoch 788/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.6016 - acc: 0.9298\n",
      "Epoch 789/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.6001 - acc: 0.9280\n",
      "Epoch 790/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5975 - acc: 0.9322\n",
      "Epoch 791/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5950 - acc: 0.9368\n",
      "Epoch 792/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5953 - acc: 0.9396\n",
      "Epoch 793/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5946 - acc: 0.9344\n",
      "Epoch 794/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5938 - acc: 0.9470\n",
      "Epoch 795/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5918 - acc: 0.9460\n",
      "Epoch 796/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5971 - acc: 0.9358\n",
      "Epoch 797/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5939 - acc: 0.9420\n",
      "Epoch 798/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5943 - acc: 0.9370\n",
      "Epoch 799/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5971 - acc: 0.9312\n",
      "Epoch 800/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5968 - acc: 0.9352\n",
      "Epoch 801/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5949 - acc: 0.9384\n",
      "Epoch 802/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5942 - acc: 0.9392\n",
      "Epoch 803/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5947 - acc: 0.9392\n",
      "Epoch 804/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5941 - acc: 0.9392\n",
      "Epoch 805/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5952 - acc: 0.9372\n",
      "Epoch 806/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.6001 - acc: 0.9290\n",
      "Epoch 807/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5988 - acc: 0.9330\n",
      "Epoch 808/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5931 - acc: 0.9418\n",
      "Epoch 809/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5937 - acc: 0.9396\n",
      "Epoch 810/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5939 - acc: 0.9380\n",
      "Epoch 811/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5937 - acc: 0.9454\n",
      "Epoch 812/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5934 - acc: 0.9444\n",
      "Epoch 813/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5940 - acc: 0.9350\n",
      "Epoch 814/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5957 - acc: 0.9378\n",
      "Epoch 815/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5997 - acc: 0.9274\n",
      "Epoch 816/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5969 - acc: 0.9346\n",
      "Epoch 817/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5950 - acc: 0.9408\n",
      "Epoch 818/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5970 - acc: 0.9414\n",
      "Epoch 819/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5944 - acc: 0.9430\n",
      "Epoch 820/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5961 - acc: 0.9362\n",
      "Epoch 821/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5945 - acc: 0.9434\n",
      "Epoch 822/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5944 - acc: 0.9400\n",
      "Epoch 823/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5952 - acc: 0.9380\n",
      "Epoch 824/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5949 - acc: 0.9352\n",
      "Epoch 825/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5941 - acc: 0.9472\n",
      "Epoch 826/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5958 - acc: 0.9434\n",
      "Epoch 827/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5952 - acc: 0.9352\n",
      "Epoch 828/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5944 - acc: 0.9404\n",
      "Epoch 829/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5983 - acc: 0.9364\n",
      "Epoch 830/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5956 - acc: 0.9376\n",
      "Epoch 831/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5959 - acc: 0.9376\n",
      "Epoch 832/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5972 - acc: 0.9348\n",
      "Epoch 833/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5958 - acc: 0.9404\n",
      "Epoch 834/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5973 - acc: 0.9366\n",
      "Epoch 835/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5953 - acc: 0.9402\n",
      "Epoch 836/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5947 - acc: 0.9392\n",
      "Epoch 837/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5933 - acc: 0.9420\n",
      "Epoch 838/1000\n",
      "5000/5000 [==============================] - 0s 89us/step - loss: 0.5941 - acc: 0.9436\n",
      "Epoch 839/1000\n",
      "5000/5000 [==============================] - 1s 124us/step - loss: 0.5949 - acc: 0.9414\n",
      "Epoch 840/1000\n",
      "5000/5000 [==============================] - 1s 122us/step - loss: 0.5938 - acc: 0.9442\n",
      "Epoch 841/1000\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 0.5959 - acc: 0.9376\n",
      "Epoch 842/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5950 - acc: 0.9396\n",
      "Epoch 843/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.5939 - acc: 0.9394\n",
      "Epoch 844/1000\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 0.5949 - acc: 0.9394\n",
      "Epoch 845/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5972 - acc: 0.9316\n",
      "Epoch 846/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5975 - acc: 0.9348\n",
      "Epoch 847/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.5949 - acc: 0.9388\n",
      "Epoch 848/1000\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 0.5958 - acc: 0.9390\n",
      "Epoch 849/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5942 - acc: 0.9364\n",
      "Epoch 850/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5929 - acc: 0.9458\n",
      "Epoch 851/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5937 - acc: 0.9404\n",
      "Epoch 852/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.6081 - acc: 0.9226\n",
      "Epoch 853/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5972 - acc: 0.9308\n",
      "Epoch 854/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5988 - acc: 0.9360\n",
      "Epoch 855/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5945 - acc: 0.9414\n",
      "Epoch 856/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5951 - acc: 0.9404\n",
      "Epoch 857/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5959 - acc: 0.9436\n",
      "Epoch 858/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5949 - acc: 0.9362\n",
      "Epoch 859/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.5932 - acc: 0.9430\n",
      "Epoch 860/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.5937 - acc: 0.9440\n",
      "Epoch 861/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5941 - acc: 0.9476\n",
      "Epoch 862/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5935 - acc: 0.9444\n",
      "Epoch 863/1000\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 0.5942 - acc: 0.9342\n",
      "Epoch 864/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5948 - acc: 0.9428\n",
      "Epoch 865/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5938 - acc: 0.9424\n",
      "Epoch 866/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5946 - acc: 0.9366\n",
      "Epoch 867/1000\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5975 - acc: 0.9352\n",
      "Epoch 868/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5977 - acc: 0.9310\n",
      "Epoch 869/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.5961 - acc: 0.9366\n",
      "Epoch 870/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5948 - acc: 0.9362\n",
      "Epoch 871/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5940 - acc: 0.9400\n",
      "Epoch 872/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5954 - acc: 0.9426\n",
      "Epoch 873/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5953 - acc: 0.9416\n",
      "Epoch 874/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5948 - acc: 0.9408\n",
      "Epoch 875/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5955 - acc: 0.9370\n",
      "Epoch 876/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5969 - acc: 0.9376\n",
      "Epoch 877/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5963 - acc: 0.9364\n",
      "Epoch 878/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5971 - acc: 0.9382\n",
      "Epoch 879/1000\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 0.5957 - acc: 0.9372\n",
      "Epoch 880/1000\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 0.5950 - acc: 0.9364\n",
      "Epoch 881/1000\n",
      "5000/5000 [==============================] - 0s 70us/step - loss: 0.5939 - acc: 0.9402\n",
      "Epoch 882/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 72us/step - loss: 0.5941 - acc: 0.9454\n",
      "Epoch 883/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5949 - acc: 0.9446\n",
      "Epoch 884/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5957 - acc: 0.9336\n",
      "Epoch 885/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5956 - acc: 0.9346\n",
      "Epoch 886/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5963 - acc: 0.9346\n",
      "Epoch 887/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5956 - acc: 0.9360\n",
      "Epoch 888/1000\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 0.5942 - acc: 0.9400\n",
      "Epoch 889/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5936 - acc: 0.9450\n",
      "Epoch 890/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5941 - acc: 0.9394\n",
      "Epoch 891/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5937 - acc: 0.9396\n",
      "Epoch 892/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5963 - acc: 0.9408\n",
      "Epoch 893/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5946 - acc: 0.9422\n",
      "Epoch 894/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5940 - acc: 0.9404\n",
      "Epoch 895/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5939 - acc: 0.9408\n",
      "Epoch 896/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5953 - acc: 0.9386\n",
      "Epoch 897/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5950 - acc: 0.9418\n",
      "Epoch 898/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5970 - acc: 0.9330\n",
      "Epoch 899/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5958 - acc: 0.9362\n",
      "Epoch 900/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5951 - acc: 0.9378\n",
      "Epoch 901/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5944 - acc: 0.9400\n",
      "Epoch 902/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5952 - acc: 0.9366\n",
      "Epoch 903/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5938 - acc: 0.9398\n",
      "Epoch 904/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5946 - acc: 0.9368\n",
      "Epoch 905/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5942 - acc: 0.9416\n",
      "Epoch 906/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5939 - acc: 0.9398\n",
      "Epoch 907/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5956 - acc: 0.9352\n",
      "Epoch 908/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5970 - acc: 0.9320\n",
      "Epoch 909/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5955 - acc: 0.9352\n",
      "Epoch 910/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5951 - acc: 0.9352\n",
      "Epoch 911/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5970 - acc: 0.9266\n",
      "Epoch 912/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5980 - acc: 0.9364\n",
      "Epoch 913/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5982 - acc: 0.9310\n",
      "Epoch 914/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5951 - acc: 0.9418\n",
      "Epoch 915/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5937 - acc: 0.9446\n",
      "Epoch 916/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5941 - acc: 0.9424\n",
      "Epoch 917/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5929 - acc: 0.9460\n",
      "Epoch 918/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5932 - acc: 0.9412\n",
      "Epoch 919/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5941 - acc: 0.9430\n",
      "Epoch 920/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5956 - acc: 0.9310\n",
      "Epoch 921/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5960 - acc: 0.9384\n",
      "Epoch 922/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5958 - acc: 0.9404\n",
      "Epoch 923/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5964 - acc: 0.9340\n",
      "Epoch 924/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5936 - acc: 0.9410\n",
      "Epoch 925/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5951 - acc: 0.9350\n",
      "Epoch 926/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5972 - acc: 0.9344\n",
      "Epoch 927/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5981 - acc: 0.9306\n",
      "Epoch 928/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5994 - acc: 0.9292\n",
      "Epoch 929/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5933 - acc: 0.9404\n",
      "Epoch 930/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5922 - acc: 0.9450\n",
      "Epoch 931/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5928 - acc: 0.9442\n",
      "Epoch 932/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5924 - acc: 0.9440\n",
      "Epoch 933/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5958 - acc: 0.9416\n",
      "Epoch 934/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5940 - acc: 0.9424\n",
      "Epoch 935/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5955 - acc: 0.9366\n",
      "Epoch 936/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5953 - acc: 0.9428\n",
      "Epoch 937/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5955 - acc: 0.9394\n",
      "Epoch 938/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5956 - acc: 0.9324\n",
      "Epoch 939/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5949 - acc: 0.9404\n",
      "Epoch 940/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5943 - acc: 0.9398\n",
      "Epoch 941/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5936 - acc: 0.9410\n",
      "Epoch 942/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5943 - acc: 0.9396\n",
      "Epoch 943/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5985 - acc: 0.9290\n",
      "Epoch 944/1000\n",
      "5000/5000 [==============================] - 0s 69us/step - loss: 0.5935 - acc: 0.9388\n",
      "Epoch 945/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5920 - acc: 0.9462\n",
      "Epoch 946/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5944 - acc: 0.9408\n",
      "Epoch 947/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5949 - acc: 0.9400\n",
      "Epoch 948/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5945 - acc: 0.9378\n",
      "Epoch 949/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5939 - acc: 0.9398\n",
      "Epoch 950/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5946 - acc: 0.9402\n",
      "Epoch 951/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5973 - acc: 0.9336\n",
      "Epoch 952/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5967 - acc: 0.9358\n",
      "Epoch 953/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5949 - acc: 0.9388\n",
      "Epoch 954/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5960 - acc: 0.9356\n",
      "Epoch 955/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5941 - acc: 0.9404\n",
      "Epoch 956/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5930 - acc: 0.9392\n",
      "Epoch 957/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5949 - acc: 0.9354\n",
      "Epoch 958/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5946 - acc: 0.9458\n",
      "Epoch 959/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5957 - acc: 0.9308\n",
      "Epoch 960/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5942 - acc: 0.9414\n",
      "Epoch 961/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5935 - acc: 0.9424\n",
      "Epoch 962/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5944 - acc: 0.9448\n",
      "Epoch 963/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5958 - acc: 0.9344\n",
      "Epoch 964/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5965 - acc: 0.9374\n",
      "Epoch 965/1000\n",
      "5000/5000 [==============================] - 0s 62us/step - loss: 0.5963 - acc: 0.9372\n",
      "Epoch 966/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5951 - acc: 0.9396\n",
      "Epoch 967/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5934 - acc: 0.9400\n",
      "Epoch 968/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5940 - acc: 0.9364\n",
      "Epoch 969/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5945 - acc: 0.9412\n",
      "Epoch 970/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5946 - acc: 0.9358\n",
      "Epoch 971/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5930 - acc: 0.9424\n",
      "Epoch 972/1000\n",
      "5000/5000 [==============================] - 0s 63us/step - loss: 0.5936 - acc: 0.9402\n",
      "Epoch 973/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5942 - acc: 0.9444\n",
      "Epoch 974/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5935 - acc: 0.9398\n",
      "Epoch 975/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5932 - acc: 0.9422\n",
      "Epoch 976/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5964 - acc: 0.9348\n",
      "Epoch 977/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5954 - acc: 0.9380\n",
      "Epoch 978/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5946 - acc: 0.9366\n",
      "Epoch 979/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5990 - acc: 0.9278\n",
      "Epoch 980/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5947 - acc: 0.9418\n",
      "Epoch 981/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5961 - acc: 0.9364\n",
      "Epoch 982/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5937 - acc: 0.9394\n",
      "Epoch 983/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5947 - acc: 0.9386\n",
      "Epoch 984/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5937 - acc: 0.9430\n",
      "Epoch 985/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5940 - acc: 0.9406\n",
      "Epoch 986/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5932 - acc: 0.9430\n",
      "Epoch 987/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5937 - acc: 0.9400\n",
      "Epoch 988/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5965 - acc: 0.9400\n",
      "Epoch 989/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5938 - acc: 0.9404\n",
      "Epoch 990/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5944 - acc: 0.9388\n",
      "Epoch 991/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5954 - acc: 0.9398\n",
      "Epoch 992/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5931 - acc: 0.9388\n",
      "Epoch 993/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5944 - acc: 0.9372\n",
      "Epoch 994/1000\n",
      "5000/5000 [==============================] - 0s 64us/step - loss: 0.5976 - acc: 0.9380\n",
      "Epoch 995/1000\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 0.5947 - acc: 0.9370\n",
      "Epoch 996/1000\n",
      "5000/5000 [==============================] - 0s 67us/step - loss: 0.5936 - acc: 0.9464\n",
      "Epoch 997/1000\n",
      "5000/5000 [==============================] - 0s 66us/step - loss: 0.5951 - acc: 0.9428\n",
      "Epoch 998/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5957 - acc: 0.9346\n",
      "Epoch 999/1000\n",
      "5000/5000 [==============================] - 0s 65us/step - loss: 0.5943 - acc: 0.9400\n",
      "Epoch 1000/1000\n",
      "5000/5000 [==============================] - 0s 68us/step - loss: 0.5938 - acc: 0.9388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70c439b128>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x,train_y,epochs=1000,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_image(image_path):\n",
    "    image_vector = encode(image_path)\n",
    "    image_vector_1 = np.array([image_vector])\n",
    "    print(image_vector_1.shape)\n",
    "    probas = model.predict(image_vector_1)\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2048)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.1891487e-05, 8.7331643e-04, 1.7074140e-04, 5.4652702e-02,\n",
       "       8.6284279e-05, 2.1374390e-04, 1.8307652e-05, 9.4345260e-01,\n",
       "       3.9821817e-04, 1.1218027e-04], dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test_image(\"tyagi.jpg\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bowl': 0.02197122,\n",
       " 'eat': 0.026833424,\n",
       " 'food': 0.03123857,\n",
       " 'glass': 0.031358887,\n",
       " 'group': 0.038096927,\n",
       " 'orange': 0.011274825,\n",
       " 'people': 0.052405376,\n",
       " 'sit': 0.046688832,\n",
       " 'table': 0.09352577,\n",
       " 'wine': 0.019313678}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('topic_word_prob','rb')\n",
    "g = load(f)\n",
    "g[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_words_probablity(image_path , dictionary_file_name , topic_word_file,  num_of_topics ):\n",
    "    f = open(topic_word_file , 'rb')\n",
    "    topic_word_prob = load(f)\n",
    "    all_unique_words_in_topics = []\n",
    "    final_probablity = {}\n",
    "    \n",
    "    probality_topic_image = list(predict_test_image(image_path))\n",
    "    \n",
    "    for topic_num in topic_word_prob.keys():\n",
    "        for word in topic_word_prob[topic_num].keys():\n",
    "            if word not in all_unique_words_in_topics:\n",
    "                all_unique_words_in_topics.append(word)\n",
    "                \n",
    "    for word in all_unique_words_in_topics:\n",
    "        \n",
    "        for topic in topic_word_prob.keys():\n",
    "            \n",
    "            #print(topic,word)\n",
    "            if word not in final_probablity:\n",
    "                final_probablity[word] = 0\n",
    "            \n",
    "            if word in topic_word_prob[topic]:\n",
    "                final_probablity[word] += topic_word_prob[topic][word]*probality_topic_image[0][topic]\n",
    "    \n",
    "    sorted_probs_word_text = [(k, final_probablity[k]) for k in sorted(final_probablity, key=final_probablity.get, reverse=True)]\n",
    "    return sorted_probs_word_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2048)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('people', 0.0631325771671527),\n",
       " ('fly', 0.04046446830034256),\n",
       " ('train', 0.03931855410337448),\n",
       " ('kite', 0.036848388612270355),\n",
       " ('beach', 0.033674780279397964),\n",
       " ('water', 0.030073486268520355),\n",
       " ('stand', 0.02807271176251369),\n",
       " ('group', 0.017737267633719966),\n",
       " ('walk', 0.016967670992016792),\n",
       " ('kit', 0.01684562861919403),\n",
       " ('clock', 0.0026222290471196175),\n",
       " ('tower', 0.0013110693544149399),\n",
       " ('field', 0.0011596783297136426),\n",
       " ('build', 0.0009805466197576607),\n",
       " ('grass', 0.0009250514558516443),\n",
       " ('large', 0.000921573315281421),\n",
       " ('tree', 0.0006474492838606238),\n",
       " ('fence', 0.0006206401158124208),\n",
       " ('giraffe', 0.0005468108574859798),\n",
       " ('tennis', 0.0002014602068811655),\n",
       " ('play', 0.00016040768241509795),\n",
       " ('baseball', 0.00014882029790896922),\n",
       " ('game', 0.00012661388609558344),\n",
       " ('ball', 0.0001018410621327348),\n",
       " ('hold', 9.354104315661971e-05),\n",
       " ('player', 8.464027632726356e-05),\n",
       " ('court', 6.598611798835918e-05),\n",
       " ('swing', 5.593043169938028e-05),\n",
       " ('sit', 4.524348040035875e-05),\n",
       " ('street', 2.8742746508214623e-05),\n",
       " ('phone', 2.7790280000772327e-05),\n",
       " ('bear', 2.4996215870487504e-05),\n",
       " ('laptop', 2.3391436116071418e-05),\n",
       " ('sign', 1.81228133442346e-05),\n",
       " ('desk', 1.7715234207571484e-05),\n",
       " ('park', 1.7110487476657e-05),\n",
       " ('teddy', 1.62987744261045e-05),\n",
       " ('cell', 1.5142843949433882e-05),\n",
       " ('bathroom', 1.3296395081852097e-05),\n",
       " ('city', 1.1887676009791903e-05),\n",
       " ('bench', 1.0338759238948114e-05),\n",
       " ('stuff', 1.0085897883982398e-05),\n",
       " ('toilet', 9.74490922089899e-06),\n",
       " ('woman', 9.250824703599392e-06),\n",
       " ('horse', 8.646140486234799e-06),\n",
       " ('table', 4.5441149794100966e-06),\n",
       " ('room', 3.034462906725821e-06),\n",
       " ('rid', 2.4061200747382827e-06),\n",
       " ('snow', 1.9911694835172966e-06),\n",
       " ('motorcycle', 1.940366701091989e-06),\n",
       " ('skateboard', 1.9393385173316346e-06),\n",
       " ('person', 1.8031855688604992e-06),\n",
       " ('live', 1.7917395780386869e-06),\n",
       " ('ski', 1.7754580312612234e-06),\n",
       " ('pizza', 1.7569545889273286e-06),\n",
       " ('food', 1.2793457457860313e-06),\n",
       " ('chair', 1.2448765573935816e-06),\n",
       " ('glass', 1.2174444918855443e-06),\n",
       " ('couch', 1.0856768994926824e-06),\n",
       " ('eat', 1.0417526254968834e-06),\n",
       " ('bike', 1.036717435454193e-06),\n",
       " ('board', 9.956676194633474e-07),\n",
       " ('girl', 9.86145096248947e-07),\n",
       " ('bowl', 8.529875685781008e-07),\n",
       " ('wine', 7.498139211747912e-07),\n",
       " ('child', 7.398044203910104e-07),\n",
       " ('orange', 4.3772195112978807e-07),\n",
       " ('plate', 1.1485045803283356e-07),\n",
       " ('white', 6.390088458374521e-08),\n",
       " ('flower', 5.9315834022299896e-08),\n",
       " ('vase', 5.60342883204612e-08),\n",
       " ('kitchen', 5.487700605044665e-08),\n",
       " ('refrigerator', 2.8608869939716897e-08),\n",
       " ('lay', 2.7905763033686526e-08),\n",
       " ('fill', 2.653492181536876e-08),\n",
       " ('sandwich', 2.549558786313355e-08),\n",
       " ('oven', 2.1425714535894258e-08),\n",
       " ('display', 2.011693212011778e-08),\n",
       " ('wear', 1.8106318222521622e-08),\n",
       " ('black', 1.7894649317895528e-08)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_words_probablity(\"83085.jpg\",'Dict','topic_word_prob',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
